<html>
<head>
<title>TIP8311 -- Pattern recognition -- 2015.2</title>
<link rel=StyleSheet href="style.css" type="text/css" media=screen>
</head>

<body>
<a name="top"></a>
<h1>Pattern recognition -- TIP8311 -- 2015.2</h1>

<table style="width: 100%;"><tr>
<td>
<bold>Instructor:</bold><br>
<a href="https://users.ics.aalto.fi/fcorona">Francesco</a> (F)
(francesco dot corona at aalto dot fi)<br>

<br>
<bold>Teaching assistants:</bold><br>
<a href="http://lattes.cnpq.br/9488773349496618">Rafael</a> (R)
(rafael dot de dot oliveira dot e dot lima  at gmail dot com)<br>
<a href="http://lattes.cnpq.br/6231055876896424">Edmilson</a> (E)
(eqfilho at sfiec dot org dot br)<br>
<br>
<bold>Lecture times:</bold>
Tuesdays and Thursdays 2-4 pm<br>
<bold>Lecture places:</bold>
Bloco 726, Sala 14<br>
<br>
<bold>Material:</bold>
Course slides will suffice.<br><br> The material in the slides can be complemented using material from the following textbooks (list not exhaustive).
<ol>
<li> <bold>Pattern Recognition and Machine Learning</bold> <br> Christopher M. Bishop
<li> <bold>Machine Learning: A Probabilistic Perspective</bold> <br> Kevin P. Murphy
<li> <bold>The Elements of Statistical Learning</bold> (<a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">Online</a>) <br> Trevor Hastie, Robert Tibshirani and Jerome Friedman
<li> <bold>Neural Networks</bold> (2nd edition) <br> Simon Haykin
<li> <bold>Learning with Kernels</bold> <br> Bernhard Sch&ouml;lkopf and Alexander J. Smola
<li> <bold>Gaussian Processes for Machine Learning</bold> (<a href="http://www.gaussianprocess.org/gpml/">Online</a>) <br> Edward Rasmussen and Christopher K. I. Williams
<li> <bold>Pattern Recognition</bold> (2nd edition) <br> Richard O. Duda, Peter E. Hart and David G. Stork
</ol>
The material in the slides is mostly based on book [1]. <!--Pirated copies of these books are floating the web.--><br>
<br>
<bold>Results:</bold>
<font color="red">The preliminary results are available.</font>
</td>
    
<td style="width: 50%; background-image: url('primdual.png');
background-position: center top;
background-repeat: no-repeat; opacity: 0.99;"></td>
</tr></table>

<br>
<hr id="dash">
Go to: &nbsp;
<a href="#lectures">Lectures</a> |
<a href="#assignments">Assignments</a> |
<a href="#schedule">Schedule</a> |
<a href="#results">Results</a> |
<br>

<hr id="dash">
<a name="lectures"></a>
<h2>Lecture notes</h2>

<ol>
<li> Introduction (F)<ul><li><a href="Lecture_notes/00_Overview.pdf">Slides</a> (AUG 20)</ul>
<li> Probability theory (F)<ul><li><a href="Lecture_notes/01_1_Probability_theory.pdf">Slides</a> (AUG 25)</ul>
<li> Decision theory (F)<ul><li><a href="Lecture_notes/01_2_Decision_theory.pdf">Slides</a> (AUG 27)</ul>
<li> Information theory (F)<ul><li><a href="Lecture_notes/01_3_Information_theory_draft_SEP01.pdf">Slides</a> (draft SEP 01)</ul>
<li> Probability distributions - Binary and multinomial variables (F)<ul><li><a href="Lecture_notes/02_1_Binary_and_multinomial_variables_draft_SEP08.pdf">Slides</a> (draft SEP 08)</ul>
<li> Probability distributions - The Gaussian distribution (F)<ul><li><a href="Lecture_notes/02_2_Gaussian_distributions_draft_SEP15.pdf">Slides</a> (draft SEP 15)</ul>
<li> Probability distributions - Non-parametric density estimation (F)<ul><li><a href="Lecture_notes/02_3_Nonparametric_density_estimation_draft_SEP17.pdf">Slides</a> (draft SEP 17)</ul>
<li> Linear models for regression - Linear basis function models (F)<ul><li><a href="Lecture_notes/03_1_Linear_basis_function_models_draft_SEP24.pdf">Slides</a> (draft SEP 24)</ul>
<li> Linear models for regression - Bias-variance trade-off (F)<ul><li><a href="Lecture_notes/03_2_Bias-variance_trade-off_draft_SEP24.pdf">Slides</a> (draft SEP 24)</ul>
<li> Linear models for regression - Bayesian linear regression (F)<ul><li><a href="Lecture_notes/03_3_Bayesian linear regression_draft_SEP29.pdf">Slides</a> (draft SEP 29)</ul>
<li> Linear models for Classification - Discriminant functions (F)<ul><li><a href="Lecture_notes/04_1_Discriminant_functions_draft_OCT06.pdf">Slides</a> (draft OCT 06)</ul>
<li> Linear models for Classification - Probabilistic generative models (F)<ul><li><a href="Lecture_notes/04_2_Generative_models_draft_OCT08.pdf">Slides</a> (draft OCT 08)</ul>
<li> Linear models for Classification - Probabilistic discriminative models (F)<ul><li><a href="Lecture_notes/04_3_Discriminative_models_draft_OCT13.pdf">Slides</a> (draft OCT 13)</ul>
<li> Neural networks - Feed-forward network functions (F)<ul><li><a href="Lecture_notes/05_1_Feedforward_network_functions_draft_OCT20.pdf">Slides</a> (draft OCT 20)</ul>
<li> Kernel methods - Representation and RBF networks (F)<ul><li><a href="Lecture_notes/06_1_Representations_kernels_RBF_draft_OCT27.pdf">Slides</a> (draft OCT 27)</ul>
<li> Kernel methods - Gaussian processes (F)<ul><li><a href="Lecture_notes/06_2_Gaussian_processes_draft_OCT29.pdf">Slides</a> (draft OCT 29)</ul>
<li> Sparse kernel methods - Maximum margin classifiers (F)<ul><li><a href="Lecture_notes/07_1_Maximum_margin_classifiers_draft_NOV05.pdf">Slides</a> (draft NOV 05)</ul>
<li> Sparse kernel methods - Support vector regression (F)<ul><li><a href="Lecture_notes/07_2_Support_vector_regression_draft_NOV10.pdf">Slides</a> (draft NOV 10)</ul>
<li> Graphical models - Directed, undirected, factor graphs (F)<ul><li><a href="Lecture_notes/08_1_Graphical_models_draft_Dec08.pdf">Slides</a> (draft DEC 08, not available yet)</ul>
</ol>

<a href="#top">Top</a>

<hr id="dash">
<a name="assignments"></a>
<h2>Assignments</h2>
As we use problem set questions covered by books, papers and webpages, we expect you not to copy, refer to, or look at the solutions in preparing your answers. This is a graduate class, we expect you to want to learn and not google for answers. The purpose of problem sets is to help you think about the material, not just give us the right answers. <br><br> If you do happen to use other material, it must be acknowledged clearly with a citation on the submitted solution. <br><br>
Homeworks must be done individually: Each of you must hand in his/her own answers. In addition, each of you must write his/her own code when requested. It is acceptable, however, for you to collaborate in figuring out answers. We are assuming that, as participants in a graduate course, you take the responsibility to make sure you personally understand the solution to any work arising from collaboration (though, you must indicate on each homework with whom you collaborated).
<ol>
<li> Assignment 1 (R and E) - Return your solutions via email to R and/or E by <s><font color="red">SEP 28 2015, 23:59:59 (Fortaleza time)</font></s>
 <ul>
 <li><a href="Assignments/01_Assignment.pdf">First assignment</a> (SEP 03) -- Solve a minimum of three (3) exercises per section (4)
 </ul>
 <br> Errata:
 <ul> Exercise 18 - Replace $H[\mathbf{x}|\mathbf{y}] = H[\mathbf{y}|\mathbf{x}] + H[\mathbf{x}]$ with $H[\mathbf{x},\mathbf{y}] = H[\mathbf{y}|\mathbf{x}] + H[\mathbf{x}]$</ul>
 <br>
<li> Assignment 2 (R and E) - Return your solutions via email to R and/or E by <s><font color="red">OCT 13 2015, 23:59:59 (Fortaleza time)</font></s>
 <ul>
 <li><a href="Assignments/02_Assignment.pdf">Second assignment</a> (SEP 22) -- Solve all of the exercises
 <li><a href="Assignments/02_Assignment_data.dat">Data</a>
 </ul>
 <br>
<li> Assignment 3 (R and E) - Return your solutions via email to R and/or E by <s><font color="red">NOV 03 2015, 23:59:59 (Fortaleza time)</font></s>
 <ul>
 <li><a href="Assignments/03_Assignment.pdf">Third assignment</a> (OCT 01) -- Solve all of the exercises
 <li><a href="Assignments/03_Assignment_data.zip">Data</a>
 </ul>
 <br>
 <li> Assignment 4 (R and E) - Return your solutions via email to R and/or E by <s><font color="red">NOV 23 (WAS: NOV 16) 2015, 23:59:59 (Fortaleza time)</font></s>
 <ul>
 <li><a href="Assignments/04_Assignment.pdf">Fourth assignment</a> (OCT 15) -- Solve all of the exercises
 <li><a href="Assignments/04_Assignment_data.zip">Data</a>
 </ul>
 <br>
 <li> Assignment 5 (R and E) - Return your solutions via email to R and/or E by <s><font color="red">DEC 07 (WAS: NOV 30) 2015, 23:59:59 (Fortaleza time)</font></s>
 <ul>
 <li><a href="Assignments/05_Assignment.pdf">Fifth assignment</a> (NOV 03) -- Solve all of the exercises
 <li><a href="Assignments/05_Assignment_data.zip">Data</a>
 </ul>
 <br>
 <li> Assignment 6 (R and E) - Return your solutions via email to R and/or E by <s><font color="red">DEC 18 2015, 23:59:59 (Fortaleza time)</font></s>
 <ul>
 <li><a href="Assignments/06_Assignment.pdf">Sixth (LAST) assignment</a> (NOV 12) -- Solve all of the exercises
 <li><a href="Assignments/06_Assignment_data.zip">Data</a>
 </ul>
</ol>

<a href="#top">Top</a>

<hr id="dash">
<a name="schedule"></a>
<h2>Schedule</h2>

Estimated class schedule: It gets defined as we roll and it is subject to change, according to time and class interests.

<br><br>
<table id="bord" style="width: 100%;">
<tr id="bord">
    <td id="bord" style="width: 15%;">THU Aug 20</td>
    <td id="bord" style="width: 50%;">00. Introduction (F)</td>
    <td id="bord" style="width: 35%;">Course introduction.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">TUE Aug 25</td>
    <td id="bord" style="width: 50%;">01. Probability theory (F)</td>
    <td id="bord" style="width: 35%;">Generalities, densities, expectations and covariances, Bayesian probabilities, the univariate Gaussian.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">THU Aug 27</td>
    <td id="bord" style="width: 50%;">02. Decision theory (F)</td>
    <td id="bord" style="width: 35%;">Generalities, misclassification rates, expected losses, loss for regression.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">TUE Sep 01</td>
    <td id="bord" style="width: 50%;">03. Information theory (F)</td>
    <td id="bord" style="width: 35%;">Generalities, entropy and differential entropy, conditional entropy, relative entropy and mutual information.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">THU Sep 03</td>
    <td id="bord" style="width: 50%;">04. Exercises (R and E)</td>
    <td id="bord" style="width: 35%;">Probability, decision and information theory.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">TUE Sep 08</td>
    <td id="bord" style="width: 50%;">05. Probability distributions (F)</td>
    <td id="bord" style="width: 35%;">The binomial distribution, Bernoulli and beta distributions and the beta prior. <br> Multinomial distributions, the generalised Bernoulli distribution and the Dirichlet prior.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">THU Sep 10</td>
    <td id="bord" style="width: 50%;">06. Probability distributions (F)</td>
    <td id="bord" style="width: 35%;">The Gaussian distribution, conditional and marginal Gaussians. </td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">TUE Sep 15</td>
    <td id="bord" style="width: 50%;">07. Probability distributions (F)</td>
    <td id="bord" style="width: 35%;">Bayes' theorem and maximum likelihood for the Gaussian.<br> Bayesian inference for the Gaussian.<br> Mixture of Gaussians.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">THU Sep 17</td>
    <td id="bord" style="width: 50%;">08. Probability distributions (F)</td>
    <td id="bord" style="width: 35%;">Non-parametric density estimation.<br> Histograms, kernel density estimation and nearest-neighbour methods. </td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">TUE Sep 22</td>
    <td id="bord" style="width: 50%;">09. Exercises (R and E)</td>
    <td id="bord" style="width: 35%;">Probability distributions.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">THU Sep 24</td>
    <td id="bord" style="width: 50%;">10. Linear models for regression (F)</td>
    <td id="bord" style="width: 35%;">Linear basis function models, maximum likelihood and least squares, regularised least squares, and multiple outputs.<br> Bias-variance decomposition. </td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">TUE Sep 29</td>
    <td id="bord" style="width: 50%;">11. Linear models for regression (F)</td>
    <td id="bord" style="width: 35%;">Bayesian linear regression, parameter distribution and predictive distribution.<br>The equivalent kernel. </td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">THU Oct 01</td>
    <td id="bord" style="width: 50%;">12. Exercises (R and E)</td>
    <td id="bord" style="width: 35%;">Linear models for regression.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">TUE Oct 06</td>
    <td id="bord" style="width: 50%;">13. Linear models for classification (F)</td>
    <td id="bord" style="width: 35%;">Discriminant functions, Fisher's linear discriminant and the perceptron.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">THU Oct 08</td>
    <td id="bord" style="width: 50%;">14. Linear models for classification (F)</td>
    <td id="bord" style="width: 35%;">Probabilistic generative models.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">TUE Oct 13</td>
    <td id="bord" style="width: 50%;">15. Linear models for classification (F)</td>
    <td id="bord" style="width: 35%;">Probabilistic discriminative models, Logistic regression and probit regression.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">THU Oct 15</td>
    <td id="bord" style="width: 50%;">16. Exercises (R and E)</td>
    <td id="bord" style="width: 35%;">Linear models for classification.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">TUE Oct 20</td>
    <td id="bord" style="width: 50%;">17. Neural networks (F)</td>
    <td id="bord" style="width: 35%;">Feed-forward network functions. <br> Network training, parameter optimisation, local quadratic approximation, gradient information and gradient descent optimisation. <br> Error back-propagation.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">THU Oct 22</td>
    <td id="bord" style="width: 50%;">18. Exercises (R and E)</td>
    <td id="bord" style="width: 35%;">Recap.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">TUE Oct 27</td>
    <td id="bord" style="width: 50%;">19. Kernel methods (F)</td>
    <td id="bord" style="width: 35%;">Dual representations and constructing kernels. <br> Radial basis functions networks and the Nadaraya-Watson model.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">THU Oct 29</td>
    <td id="bord" style="width: 50%;">20. Kernel methods (F)</td>
    <td id="bord" style="width: 35%;">Gaussian processes.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">TUE Nov 03</td>
    <td id="bord" style="width: 50%;">21. Exercises (R and E)</td>
    <td id="bord" style="width: 35%;">Kernel methods.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">THU Nov 05</td>
    <td id="bord" style="width: 50%;">22. Sparse kernel methods (F)</td>
    <td id="bord" style="width: 35%;">Maximum margin classifiers.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">TUE Nov 10</td>
    <td id="bord" style="width: 50%;">23. Sparse kernel methods (F)</td>
    <td id="bord" style="width: 35%;">Support vector regression.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">THU Nov 12</td>
    <td id="bord" style="width: 50%;">24. Exercises (R and E)</td>
    <td id="bord" style="width: 35%;">Sparse kernel methods.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">TUE Nov 17</td>
    <td id="bord" style="width: 50%;">25. Exercises (R and E)</td>
    <td id="bord" style="width: 35%;">Recap.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">THU Nov 19</td>
    <td id="bord" style="width: 50%;">26. Exercises (R and E)</td>
    <td id="bord" style="width: 35%;">Recap.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">TUE Nov 24</td>
    <td id="bord" style="width: 50%;">27. Exercises (R and E)</td>
    <td id="bord" style="width: 35%;">Recap.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">THU Nov 26</td>
    <td id="bord" style="width: 50%;">28. Exercises (R and E)</td>
    <td id="bord" style="width: 35%;">Recap.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">TUE Dec 01</td>
    <td id="bord" style="width: 50%;">29. Exercises (R and E)</td>
    <td id="bord" style="width: 35%;">Recap.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">THU Dec 03</td>
    <td id="bord" style="width: 50%;">30. Exercises (R and E)</td>
    <td id="bord" style="width: 35%;">Recap.</td></tr>
<tr id="bord">
    <td id="bord" style="width: 15%;">TUE Dec 08</td>
    <td id="bord" style="width: 50%;">31. Grafical models (F)</td>
    <td id="bord" style="width: 35%;">Introduction to undirected (Bayes networks), directed (Markov networks) and factor graphs. <br><br> <font color="red">Last lecture ya'll!</font></td></tr>
</table>

<br>
<a href="#top">Top</a>

<hr id="dash">
<a name="results"></a>
<h2>Results</h2>

Crediting students: <a href="results_matr.htm">Preliminary results</a><br>
Non-crediting students: <a href="results_extr.htm">Preliminary results</a><br>
<br>

<br>
<a href="#top">Top</a>

</body>
</html>