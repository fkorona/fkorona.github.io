<html>
    <head>
        <title>Advanced topics in artificial intelligence I (CK0146) / Pattern recognition (TIP8311)</title>
        <link rel=StyleSheet href="style.css" type="text/css" media=screen>
            </head>

    <body>
        <a name="top"></a>
        <h1>Advanced topics in artificial intelligence I (CK0146) <br> Pattern recognition (TIP8311)</h1>

        The course overviews selected topics in machine learning, pattern recognition, or was it advanced topics in artificial intelligence? To make it simpler for everybody, we shall call it data science.
        <br><br>
        <iframe src="https://player.vimeo.com/video/1048763?loop=1&color=ffffff&title=0&byline=0&portrait=0" width="800" height="449" frameborder="5"></iframe>
        <br><br>
        The course deals with some of the central principles of data science, including probabilistic modelling, density estimation and generalised linear models for regression and classification:
        <ol>
            <li> <bold>Introductory refresher</bold>: Probability theory, decision theory and information theory;
                <li> <bold>Probability distributions</bold>: Binary and multinomial variables, the Gaussian, the exponential family, non-parametric distributions;
                    <li> <bold>Linear models for regression</bold>: Linear basis function models, Bayesian linear regression, evidence approximation;
                        <li> <bold>Linear models for classification</bold>: Discriminant functions, probabilistic generative models, probabilistic discriminative models, Bayesian logistic regression.
                            </ol>
        Stuff from the past: Previous version of the course are available (internal links may need adjustment) for <a href="https://users.ics.aalto.fi/fcorona/ATAI/2015_2">2015.2</a>
        <br><br>

        <bold>Instructor <font color=#333>&#9822;</font>:</bold> <a href="https://users.ics.aalto.fi/fcorona">Francesco Corona</a> (FC), francesco d&ouml;t corona &auml;t ufc d&ouml;t br<br>
        <bold>Teaching assistants <font color=#333>&#9816;</font>:</bold> <a href="http://lattes.cnpq.br/8983074896764353">Jos&eacute; Florencio de Queiroz Neto</a> (F <font color=#333>&#9813;</font>), jfqn &auml;t ask d&ouml;t him and <a href="http://lattes.cnpq.br/5007699859608116">Alisson Sampaio de Carvalho Alencar</a> (A  <font color=#333>&#9817;</font>), asca &auml;t ask d&ouml;t him
        <br><br>
        <bold>Physical location <font color=#333>&#9820;</font>:</bold> Tuesdays and Thursdays 10:00-12:00. Bloco 951, Sala 2.<br>
        <bold>Internet location <font color=#333>&#9814;</font>:</bold> Here! Or, <a href="https://si3.ufc.br/sigaa/public/home.jsf">here</a> (CK0146) and <a href="https://si3.ufc.br/sigaa/public/home.jsf">here</a> (TIP8311) for mambojumbo related to administration.
        <br><br>
        <bold>Evaluation <font color=#333>&#9997;</font>:</bold> Approx. half a dozen theoretical and practical problem sets will be assigned as homework. Partial evaluations (APs) will consist of exercises that are randomly drawn from the aformentioned sets. The APs must be worked out in the classroom, individually.
        <br><br>
        <font color="red"> >>>>>> Wanna get candy? Participate to <a href="https://www.surveymonkey.com/r/RRBC3L8">this survery</a> and spread the link (UFC under- or post-grad peeps ONLY) <<<<<< </font> <br><br>

        <hr id="dash">
        Go to: &nbsp;
        <a href="#Lectures">Lectures and schedule</a> |
        <a href="#Assignments">Problem sets</a> |
        <a href="#Material">Supplementary material</a> |
        <a href="#Pops">As it pops out</a> |

        <hr id="dash">
        <a name="Lectures"></a>
        <h2>Lectures and schedule</h2>
        We meet on Tuesday AUG 16 at 10:15am (give or take 5), to briefly introduce each other and discuss some practicalities. <br><br>
        <ol start="0">
            <li> <bold>About this course</bold><br><br>

                <table id="bord" style="width: 100%;">
                    <tr id="bord">
                        <td id="bord" style="width: 10%;">A</td>
                        <td id="bord" style="width: 35%;"><bold>About this course</bold> (FC)
                            <ul><li><a href="Lecture_notes/00_Overview_4on1.pdf">Slides</a> (<font color=#333>&#9998;</font> AUG 30)</ul></td>
                        <td id="bord" style="width: 55%;">
                            <ul>
                                <li> About the type of machine learning, pattern recognition, and advanced topics in artificial intelligence that we study
                            </ul></td>
                    </tr>
                </table><br>
            <li> <bold>Introductory refresher</bold><br><br>

                <table id="bord" style="width: 100%;">
                    <tr id="bord">
                        <td id="bord" style="width: 10%;">A</td>
                        <td id="bord" style="width: 35%;"><bold>Probability theory</bold> (FC)
                            <ul><li><a href="Lecture_notes/01-01_Probability_refresher_4on1.pdf">Slides</a> (<font color=#333>&#9998;</font> SEP 01, <font color=#333>&#9998;</font> SEP 05, <font color=#333>&#9998;</font> SEP 08 and <font color=#333>&#9998;</font> SEP 13). <font color="red">Last updated on SEP 09</font>.</ul></td>
                        <td id="bord" style="width: 55%;">
                            <ul>
                                <li> Definitions and rules, densities, expectations and covariances
                                <li> Bayesian probabilities, the univariate Gaussian distribution
                            </ul></td>
                    </tr>
                    <tr id="bord">
                        <td id="bord" style="width: 10%;">B</td>
                        <td id="bord" style="width: 35%;"><bold>Decision theory</bold> (FC)
                            <ul><li><a href="Lecture_notes/01-02_Decision_theory_4on1.pdf">Slides</a> (<font color=#333>&#9998;</font> SEP 15)</ul></td>
                        <td id="bord" style="width: 55%;">
                            <ul>
                                <li> Minimisation of the classification rate and minimisation of the expected loss
                                <li> The reject option
                                <li> Inference and decision
                                <li> Loss functions for regression
                            </ul></td>
                    </tr>
                    <tr id="bord">
                        <td id="bord" style="width: 10%;">C</td>
                        <td id="bord" style="width: 35%;"><bold>Information theory</bold> (FC)
                            <ul><li><a href="Lecture_notes/01-03_Information_theory_4on1.pdf">Slides</a> (<font color=#333>&#9998;</font> SEP 20). <font color="red">Last updated on SEP 20</font>.</ul></td>
                        <td id="bord" style="width: 55%;">
                            <ul>
                                <li> Definitions, relative entropy and mutual information
                            </ul></td>
                    </tr>
                </table><br><a href="Exercise_sheets/CK0146_TIP8311_Exercise_01_Overview_2016_2.pdf">Exercises</a> (<font color=#333>&#9998;</font> SEP 22 and <font color=#333>&#9998;</font> SEP 27, F and A) <font color="red"><strike>Hand-in by OCT 09 (was 02) at 23:59:59 Fortaleza time</strike></font>
                <br>
                <a href="Exercise_sheets/CK0146_TIP8311_Exercise_01_Overview_2016_2.htm">Results</a> in [0,10]<br><br>
                <li> <bold>Probability distributions</bold><br><br>

                    <table id="bord" style="width: 100%;">
                        <tr id="bord">
                            <td id="bord" style="width: 10%;">A</td>
                            <td id="bord" style="width: 35%;"><bold>Binary and multinomial variables</bold> (FC)
                                <ul><li><a href="Lecture_notes/02-01_Binary_and_multinomial_variables_4on1.pdf">Slides</a> (<font color=#333>&#9998;</font> SEP 29).</ul></td>
                            <td id="bord" style="width: 55%;">
                                <ul>
                                    <li> Bernoulli, binomial and beta distributions
                                        <li> Multinomial and Dirichlet distributions
                                            </ul></td>
                        </tr>
                        <tr id="bord">
                            <td id="bord" style="width: 10%;">B</td>
                            <td id="bord" style="width: 35%;"><bold>The Gaussian distribution</bold> (FC)
                                <ul><li><a href="Lecture_notes/02-02_Gaussian_variables_4on1.pdf">Slides</a> (<font color=#333>&#9998;</font> OCT 04, <font color=#333>&#9998;</font> OCT 06, <font color=#333>&#9998;</font> OCT 11, <font color=#333>&#9998;</font> OCT 13, <font color=#333>&#9998;</font> OCT 18, <font color=#333>&#9998;</font> OCT 20 and <font color=#333>&#9998;</font> OCT 25). <font color="red">Last updated on OCT 13</font></ul></td>
                            <td id="bord" style="width: 55%;">
                                <ul>
                                    <li> Conditional and marginal Gaussians, Bayes' rule for Gaussians, maximum likelihood, sequential estimation, Bayesian inference, Student's t-distribution, mixtures of Gaussians (naive MLE, with latent variables and EM, k-means)
                                        </ul></td>
                        </tr>
                        <tr id="bord">
                            <td id="bord" style="width: 10%;">C</td>
                            <td id="bord" style="width: 35%;"><bold>The exponential family</bold> (FC)</td>
                            <td id="bord" style="width: 55%;">
                                <ul>
                                    <li> Maximum likelihood and sufficient statistics
                                        <li> Conjugate and non-informative priors
                                            </ul></td>
                        </tr>
                        <tr id="bord">
                            <td id="bord" style="width: 10%;">D</td>
                            <td id="bord" style="width: 35%;"><bold>Non-parametric distributions</bold> (FC)
                                <ul><li><a href="Lecture_notes/02-03_Nonparametric_densities_4on1.pdf">Slides</a> (<font color=#333>&#9998;</font> OCT 27).  <font color="red">Last updated on NOV 02</font></ul></td>
                            <td id="bord" style="width: 55%;">
                                <ul>
                                    <li> Kernel and nearest-neighbour density estimators
                                        </ul></td>
                        </tr>
                    </table><br><a href="Exercise_sheets/CK0146_TIP8311_Exercise_02_Distributions_2016_2.pdf">Exercises</a> (<font color=#333>&#9998;</font> NOV 01 and <font color=#333>&#9998;</font> NOV 03, F and A) <font color="red"><strike>Hand-in by NOV 23 (was 16) at 23:59:59 Fortaleza time</strike></font><br><br>
                    <li> <bold>Generalised linear models for regression</bold><br><br>

                        <table id="bord" style="width: 100%;">
                            <tr id="bord">
                                <td id="bord" style="width: 10%;">A</td>
                                <td id="bord" style="width: 35%;"><bold>Linear basis function models</bold> (FC)
                                    <ul><li><a href="Lecture_notes/03-01_Linear_basis_function_models_4on1.pdf">Slides</a> (<font color=#333>&#9998;</font> NOV 08, <font color=#333>&#9998;</font> NOV 10 and <font color=#333>&#9998;</font> NOV 17). </ul></td>
                                <td id="bord" style="width: 55%;">
                                    <ul>
                                        <li> Maximum likelihood and least-squares, geometry of the least-squares
                                            <li> Sequential learning
                                                <li> Regularised least-squares
                                                    <li> Multiple outputs
                                                        </ul></td>
                            </tr>
                            <tr id="bord">
                                <td id="bord" style="width: 10%;">B</td>
                                <td id="bord" style="width: 35%;"><bold>Bias-variance decomposition</bold> (FC)
                                    <ul><li><a href="Lecture_notes/03-02_Bias_variance_decomposition_4on1.pdf">Slides</a> (<font color=#333>&#9998;</font> NOV 17). </ul></td>
                                <td id="bord" style="width: 55%;">
                                    <ul>
                                        <li> Bias-variance decomposition
                                            </ul></td>
                            </tr>
                            <tr id="bord">
                                <td id="bord" style="width: 10%;">C</td>
                                <td id="bord" style="width: 35%;"><bold>Bayesian linear regression</bold> (FC)
                                    <ul><li><a href="Lecture_notes/03-03_Bayesian_linear_regression_4on1.pdf">Slides</a> (<font color=#333>&#9998;</font> NOV 22 and <font color=#333>&#9998;</font> NOV 24). </ul></td>
                                <td id="bord" style="width: 55%;">
                                    <ul>
                                        <li> Parameter distribution and predictive distribution
                                            <li> The equivalent kernel and Gaussian processes for regression
                                                </ul></td>
                            </tr>
                            <tr id="bord">
                                <td id="bord" style="width: 10%;">D</td>
                                <td id="bord" style="width: 35%;"><bold>Bayesian model comparison</bold> (FC)</td>
                                <td id="bord" style="width: 55%;">
                                    <ul>
                                        <li> Bayesian model comparison
                                            </ul></td>
                            </tr>
                        </table><br><a href="Exercise_sheets/CK0146_TIP8311_Exercise_03_LMFR_2016_2.pdf">Exercises</a> (<font color=#333>&#9998;</font> DEC 01, F and A) <font color="red">Hand-in by DEC 13 (was DEC 11) at 23:59:59 Fortaleza time</font><br><br>
                        </table><br>
                        <li> <bold>Generalised linear models for classification</bold><br><br>

                            <table id="bord" style="width: 100%;">
                                <tr id="bord">
                                    <td id="bord" style="width: 10%;">A</td>
                                    <td id="bord" style="width: 35%;"><bold>Discriminative functions</bold> (FC)
                                        <ul><li><a href="Lecture_notes/04-01_Discriminant_functions_4on1.pdf">Slides</a> (<font color=#333>&#9998;</font> DEC 08). </ul></td>
                                    <td id="bord" style="width: 55%;">
                                        <ul>
                                            <li> Two- and multi-class classification
                                                <li> Least-squares for classification
                                                    <li> Fisher's linear discriminant
                                                        </ul></td>
                                </tr>
                                <tr id="bord">
                                    <td id="bord" style="width: 10%;">B</td>
                                    <td id="bord" style="width: 35%;"><bold>Probabilistic generative models</bold> (FC)
                                        <ul><li><a href="Lecture_notes/04-02_Generative_models_4on1.pdf">Slides</a> (<font color=#333>&#9998;</font> DEC 13). </ul></td>
                                    <td id="bord" style="width: 55%;">
                                        <ul>
                                            <li> Continuous inputs, maximum likelihood solution
                                                    </ul></td>
                                </tr>
                                <tr id="bord">
                                    <td id="bord" style="width: 10%;">C</td>
                                    <td id="bord" style="width: 35%;"><bold>Probabilistic discriminative models</bold> (FC)
                                        <ul><li><a href="Lecture_notes/04_03_Discriminative_models_4on1.pdf">Slides</a> (<font color=#333>&#9998;</font> DEC 17). </ul></td>
                                    <td id="bord" style="width: 55%;">
                                        <ul>
                                            <li> Logistic regression and iterative re-weighted least-squares
                                                <li> Probit regression
                                                    <li> Canonical link functions
                                                        </ul></td>
                                </tr>
                                <tr id="bord">
                                    <td id="bord" style="width: 10%;">D</td>
                                    <td id="bord" style="width: 35%;"><bold>Laplace approximation</bold> (FC)</td>
                                    <td id="bord" style="width: 55%;">
                                        <ul>
                                            <li> Model comparison and BIC
                                                </ul></td>
                                </tr>
                                <tr id="bord">
                                    <td id="bord" style="width: 10%;">E</td>
                                    <td id="bord" style="width: 35%;"><bold>Bayesian logistic regression</bold> (FC)</td>
                                    <td id="bord" style="width: 55%;">
                                        <ul>
                                            <li> Laplace approximation, predictive distribution
                                                </ul></td>
                                </tr>
                            </table>
                            </ol>
        [<a href="#top">Top</a>]
        <hr id="dash">

        <a name="Assignments"></a>
        <h2>Problem sets</h2>
        As we use problem set questions covered by books, papers and webpages, we expect you not to copy, refer to, or look at the solutions in preparing your answers. We expect you to want to learn and not google for answers: If you do happen to use other material, it must be acknowledged clearly with a citation on the submitted solution.<br><br> <font color=#333>&#9775;</font> The purpose of problem sets is to help you think about the material, not just give us the right answers.<br><br>

        Homeworks must be done individually: Each of you must hand in his/her own answers. In addition, each of you must write his/her own code when requested. It is acceptable, however, for you to collaborate in figuring out answers. We are assuming that you take the responsibility to make sure you personally understand the solution to any work arising from collaboration (though, you must indicate on each homework with whom you collaborated).
        <br><br>
        <font color=#333>&#9883;</font> To typeset  assignments, students and teaching assistants are encouraged to use this <a href="https://en.wikipedia.org/wiki/LaTeX">LaTeX</a> template: <a href="Additional_stuff/CK0146_TIP8311_Exercise_template_2016_2.tex">Source</a> (<a href="Additional_stuff/CK0146_TIP8311_Exercise_template_2016_2.pdf">PDF</a>).
        <br><br>
        <font color=#333>&#9888;</font> <font color="red">Assignments must be returned before deadline via SIGAA (if you're in it, you'll get notified of the opening of a new task) or, if you're not in SIGAA, return via email to one of the responsible teaching assistants - Delays will be penalised.</font>
        <br><br>
        [<a href="#top">Top</a>]
        <hr id="dash">

        <a name="Material"></a>
        <h2>Material</h2>
        Course slides will suffice. Slides are mostly based on the following textbook:
        <ul>
            <li> <bold>Pattern Recognition and Machine Learning</bold>, by Christopher Bishop.
                </ul>
        The material can be complemented using material from the following textbooks (list not exhaustive):
        <ol>
            <li> <bold>Machine Learning: A Probabilistic Perspective</bold>, by Kevin Murphy;
                <li> <bold>The Elements of Statistical Learning</bold> (<a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">Book website</a>), by Trevor Hastie, Robert Tibshirani and Jerome Friedman;
                    <li> <bold>Bayesian Reasoning and Machine Learning</bold> (<a href="http://www0.cs.ucl.ac.uk/staff/d.barber/brml">Book website</a>), by David Barber.
                        </ol>
        <font color=#333>&#9760;</font> Copies of these books are floating around.<br><br>
        <font color="red"> >>>>>> Course material is prone to a typo or two - Please inbox  FC to report <<<<<< </font> <br><br>

        <a href="#top">Top</a>
        <hr id="dash">

        <a name="Pops"></a>
        <h2>Read me or watch me</h2>
        <ul>
            <li> <a href="http://slatestarcodex.com/2016/09/12/its-bayes-all-the-way-up/">It's Bayes all the way up</a>
                <li> ...
                    </ul>

        <a href="#top">Top</a>
        <hr id="dash">

    </body>
</html>
